# @package _global_

# to execute this experiment run:
# python train.py experiment=filter

defaults:
  - override /data: landmark
  - override /model: resnet
  - override /callbacks: filter
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["filter", "resnet18"]

seed: 59

# ckpt_path: /home/quangster/Workspace/filter-tiktok/facial-landmarks/logs/train/runs/2024-10-25_20-56-52/checkpoints/epoch_008.ckpt

# ------------------override configs/trainer------------------
trainer:
  _target_: lightning.pytorch.trainer.Trainer

  default_root_dir: ${paths.output_dir}

  min_epochs: 1 # prevents early stopping
  max_epochs: 15

  accelerator: cpu
  devices: 1

  # mixed precision for extra speed-up
  # precision: 16

  # perform a validation loop every N training epochs
  check_val_every_n_epoch: 1

  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: False

# ------------------override configs/model------------------
model:
  _target_: src.models.LandmarksLitModule

  net:
    _target_: src.models.components.ResNet
    model_name: resnet18
    weights: DEFAULT
    output_shape: [68, 2]

    # _target_: src.models.components.CNN
    # output_shape: [68, 2]

  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.01
    weight_decay: 0.0

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.1
    patience: 10

  # compile model for faster training with pytorch 2.0
  compile: true

# ------------------override configs/data------------------
data:
  _target_: src.data.LandmarksDataModule
  data_dir: ${paths.data_dir}/ibug_300W_large_face_landmark_dataset
  batch_size: 64
  train_val_split: [6000, 666]
  num_workers: 11
  pin_memory: false

  train_transform:
    _target_: albumentations.Compose

    keypoint_params:
      _target_: albumentations.KeypointParams
      format: "xy"
      remove_invisible: false

    transforms:
      - _target_: albumentations.Resize
        height: 256
        width: 256
        always_apply: true
      - _target_: albumentations.ToFloat
        max_value: 255
      # - _target_: albumentations.Normalize
      #   mean: [0.485, 0.456, 0.406]
      #   std: [0.229, 0.224, 0.225]
      - _target_: albumentations.pytorch.transforms.ToTensorV2

  val_transform:
    _target_: albumentations.Compose

    keypoint_params:
      _target_: albumentations.KeypointParams
      format: "xy"
      remove_invisible: false

    transforms:
      - _target_: albumentations.Resize
        height: 256
        width: 256
        always_apply: true
      - _target_: albumentations.ToFloat
        max_value: 255
      # - _target_: albumentations.Normalize
      #   mean: [0.485, 0.456, 0.406]
      #   std: [0.229, 0.224, 0.225]
      - _target_: albumentations.pytorch.transforms.ToTensorV2

# ------------------override configs/callbacks------------------

# ------------------override configs/logger------------------
logger:
  wandb:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    name: "resnet18" # name of the run (normally generated by wandb)
    save_dir: "${paths.output_dir}"
    offline: False
    id: null # pass correct id to resume experiment!
    anonymous: null # enable anonymous logging
    project: "filter"
    log_model: True # upload lightning ckpts
    prefix: "" # a string to put at the beginning of metric keys
    # entity: "" # set to name of your wandb team
    group: ""
    tags: []
    job_type: ""
